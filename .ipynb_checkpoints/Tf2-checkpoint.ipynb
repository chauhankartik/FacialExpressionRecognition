{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rYlWVkLwJPN"
   },
   "source": [
    "## Test For CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qnPEWZnGwJPQ",
    "outputId": "c087baf4-654b-4bec-d221-a7119c7e338a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtsXH5xcwJPT"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "sOhrcPDPwJPT",
    "outputId": "2918fb12-ac5e-48fc-9850-0b40677ec6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "import os\n",
    "\n",
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 42\n",
    "\n",
    "train_data_dir = './fer2013/train'\n",
    "validation_data_dir = './fer2013/validation'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d13UwF-fwJPW"
   },
   "source": [
    "## Load and Augment Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__5PwaCvwJPW"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1B4lumq7L2yQ"
   },
   "source": [
    "## Visualizing Few Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmC7WFxFL2yS"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UxNWW1RwJPZ"
   },
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M-5XLcSKwJPZ",
    "outputId": "76bf8fbc-9179-427c-e391-fdf7bc76bbcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              18878464  \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 25,575,110\n",
      "Trainable params: 25,561,734\n",
      "Non-trainable params: 13,376\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(1024, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYiyWXABwJPc"
   },
   "source": [
    "## Specify Loss And Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "t-ZcoEdOwJPc",
    "outputId": "2427bf16-599d-4f23-e56e-50a6dbcb52b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "673/673 [==============================] - 72s 107ms/step - loss: 2.1006 - accuracy: 0.2022 - val_loss: 1.7183 - val_accuracy: 0.2483\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.71831, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 2/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.7826 - accuracy: 0.2431 - val_loss: 1.7863 - val_accuracy: 0.2700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.71831\n",
      "Epoch 3/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.7321 - accuracy: 0.2642 - val_loss: 1.6834 - val_accuracy: 0.2789\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.71831 to 1.68336, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 4/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.6739 - accuracy: 0.2965 - val_loss: 1.4223 - val_accuracy: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.68336 to 1.42226, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 5/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.5520 - accuracy: 0.3737 - val_loss: 1.6162 - val_accuracy: 0.4207\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42226\n",
      "Epoch 6/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.4630 - accuracy: 0.4160 - val_loss: 1.4646 - val_accuracy: 0.4525\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.42226\n",
      "Epoch 7/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.3988 - accuracy: 0.4460 - val_loss: 1.4498 - val_accuracy: 0.4765\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.42226\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.3212 - accuracy: 0.4822 - val_loss: 1.2620 - val_accuracy: 0.4834\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.42226 to 1.26199, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 9/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2925 - accuracy: 0.4953 - val_loss: 1.1624 - val_accuracy: 0.4928\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.26199 to 1.16244, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 10/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2755 - accuracy: 0.5036 - val_loss: 1.5175 - val_accuracy: 0.5003\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.16244\n",
      "Epoch 11/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.2536 - accuracy: 0.5082 - val_loss: 1.3543 - val_accuracy: 0.4897\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.16244\n",
      "Epoch 12/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2431 - accuracy: 0.5155 - val_loss: 1.0963 - val_accuracy: 0.5086\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.16244 to 1.09633, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 13/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2299 - accuracy: 0.5217 - val_loss: 1.3390 - val_accuracy: 0.5166\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.09633\n",
      "Epoch 14/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2180 - accuracy: 0.5294 - val_loss: 1.3394 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.09633\n",
      "Epoch 15/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.2134 - accuracy: 0.5309 - val_loss: 1.3380 - val_accuracy: 0.5347\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.09633\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 16/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1939 - accuracy: 0.5369 - val_loss: 1.0146 - val_accuracy: 0.5427\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.09633 to 1.01460, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 17/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1835 - accuracy: 0.5420 - val_loss: 1.2415 - val_accuracy: 0.5352\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.01460\n",
      "Epoch 18/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1733 - accuracy: 0.5464 - val_loss: 1.2371 - val_accuracy: 0.5369\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.01460\n",
      "Epoch 19/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1715 - accuracy: 0.5484 - val_loss: 1.0525 - val_accuracy: 0.5381\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.01460\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 20/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1703 - accuracy: 0.5505 - val_loss: 1.2977 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.01460\n",
      "Epoch 21/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1723 - accuracy: 0.5528 - val_loss: 1.6898 - val_accuracy: 0.5372\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.01460\n",
      "Epoch 22/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1660 - accuracy: 0.5493 - val_loss: 1.3558 - val_accuracy: 0.5404\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.01460\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 23/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1628 - accuracy: 0.5514 - val_loss: 1.5671 - val_accuracy: 0.5467\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.01460\n",
      "Epoch 24/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1602 - accuracy: 0.5485 - val_loss: 1.6039 - val_accuracy: 0.5341\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.01460\n",
      "Epoch 25/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1648 - accuracy: 0.5511 - val_loss: 1.3103 - val_accuracy: 0.5507\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.01460\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 26/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1637 - accuracy: 0.5498 - val_loss: 1.5395 - val_accuracy: 0.5418\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.01460\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"./emotion_custom_vgg.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 10,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzyiY1bawJPf"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "colab_type": "code",
    "id": "aoH5OvNAwJPf",
    "outputId": "83589732-03da-42b3-cd94-983dd23bf1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[237  34  19  73 113  15]\n",
      " [161  49  27  92 109  90]\n",
      " [ 33  13 748  48  19  18]\n",
      " [119  28 152 144 110  73]\n",
      " [ 94  17  50 155 268  10]\n",
      " [ 28  24  30  23   2 309]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.35      0.48      0.41       491\n",
      "        Fear       0.30      0.09      0.14       528\n",
      "       Happy       0.73      0.85      0.79       879\n",
      "     Neutral       0.27      0.23      0.25       626\n",
      "         Sad       0.43      0.45      0.44       594\n",
      "    Surprise       0.60      0.74      0.66       416\n",
      "\n",
      "    accuracy                           0.50      3534\n",
      "   macro avg       0.45      0.48      0.45      3534\n",
      "weighted avg       0.47      0.50      0.47      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CReutMRyL2yn"
   },
   "source": [
    "## Loading Saved Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zx8BKDpF7VQY"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('./emotion_custom_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEvbXuVYL2yr"
   },
   "source": [
    "## Testing the trained network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "h2B79oxJ-f7w",
    "outputId": "c06705f5-7cd2-4baa-d3cb-96c3152e64ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKir7ntHwJPi"
   },
   "source": [
    "## Visualizing Sample Validation Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "VDDDT1N_wjJ8",
    "outputId": "54fff4e3-8a8a-48f5-e2ec-c661e421da13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "ename": "DisabledFunctionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-e788530ca8f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mdraw_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-e788530ca8f1>\u001b[0m in \u001b[0;36mdraw_test\u001b[0;34m(name, pred, im, true_label)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predited - \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true - \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './fer2013/validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2U6JvFj98R_y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zeo8coopNKc8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
