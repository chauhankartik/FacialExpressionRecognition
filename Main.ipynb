{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rYlWVkLwJPN"
   },
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qnPEWZnGwJPQ",
    "outputId": "c087baf4-654b-4bec-d221-a7119c7e338a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtsXH5xcwJPT"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "sOhrcPDPwJPT",
    "outputId": "2918fb12-ac5e-48fc-9850-0b40677ec6fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "import os\n",
    "\n",
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 42\n",
    "\n",
    "train_data_dir = r'D:\\SummerInternship\\fer2013\\train'\n",
    "validation_data_dir = r'D:\\SummerInternship\\fer2013\\validation'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d13UwF-fwJPW"
   },
   "source": [
    "## Loading Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__5PwaCvwJPW"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UxNWW1RwJPZ"
   },
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M-5XLcSKwJPZ",
    "outputId": "76bf8fbc-9179-427c-e391-fdf7bc76bbcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              18878464  \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 25,575,110\n",
      "Trainable params: 25,561,734\n",
      "Non-trainable params: 13,376\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(1024, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYiyWXABwJPc"
   },
   "source": [
    "## Specify Loss And Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "t-ZcoEdOwJPc",
    "outputId": "2427bf16-599d-4f23-e56e-50a6dbcb52b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "673/673 [==============================] - 72s 107ms/step - loss: 2.1006 - accuracy: 0.2022 - val_loss: 1.7183 - val_accuracy: 0.2483\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.71831, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 2/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.7826 - accuracy: 0.2431 - val_loss: 1.7863 - val_accuracy: 0.2700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.71831\n",
      "Epoch 3/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.7321 - accuracy: 0.2642 - val_loss: 1.6834 - val_accuracy: 0.2789\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.71831 to 1.68336, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 4/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.6739 - accuracy: 0.2965 - val_loss: 1.4223 - val_accuracy: 0.3577\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.68336 to 1.42226, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 5/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.5520 - accuracy: 0.3737 - val_loss: 1.6162 - val_accuracy: 0.4207\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42226\n",
      "Epoch 6/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.4630 - accuracy: 0.4160 - val_loss: 1.4646 - val_accuracy: 0.4525\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.42226\n",
      "Epoch 7/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.3988 - accuracy: 0.4460 - val_loss: 1.4498 - val_accuracy: 0.4765\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.42226\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.3212 - accuracy: 0.4822 - val_loss: 1.2620 - val_accuracy: 0.4834\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.42226 to 1.26199, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 9/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2925 - accuracy: 0.4953 - val_loss: 1.1624 - val_accuracy: 0.4928\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.26199 to 1.16244, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 10/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2755 - accuracy: 0.5036 - val_loss: 1.5175 - val_accuracy: 0.5003\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.16244\n",
      "Epoch 11/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.2536 - accuracy: 0.5082 - val_loss: 1.3543 - val_accuracy: 0.4897\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.16244\n",
      "Epoch 12/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2431 - accuracy: 0.5155 - val_loss: 1.0963 - val_accuracy: 0.5086\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.16244 to 1.09633, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 13/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2299 - accuracy: 0.5217 - val_loss: 1.3390 - val_accuracy: 0.5166\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.09633\n",
      "Epoch 14/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.2180 - accuracy: 0.5294 - val_loss: 1.3394 - val_accuracy: 0.5143\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.09633\n",
      "Epoch 15/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.2134 - accuracy: 0.5309 - val_loss: 1.3380 - val_accuracy: 0.5347\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.09633\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 16/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1939 - accuracy: 0.5369 - val_loss: 1.0146 - val_accuracy: 0.5427\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.09633 to 1.01460, saving model to ./emotion_custom_vgg.h5\n",
      "Epoch 17/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1835 - accuracy: 0.5420 - val_loss: 1.2415 - val_accuracy: 0.5352\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.01460\n",
      "Epoch 18/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1733 - accuracy: 0.5464 - val_loss: 1.2371 - val_accuracy: 0.5369\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.01460\n",
      "Epoch 19/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1715 - accuracy: 0.5484 - val_loss: 1.0525 - val_accuracy: 0.5381\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.01460\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 20/50\n",
      "673/673 [==============================] - 63s 93ms/step - loss: 1.1703 - accuracy: 0.5505 - val_loss: 1.2977 - val_accuracy: 0.5481\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.01460\n",
      "Epoch 21/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1723 - accuracy: 0.5528 - val_loss: 1.6898 - val_accuracy: 0.5372\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.01460\n",
      "Epoch 22/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1660 - accuracy: 0.5493 - val_loss: 1.3558 - val_accuracy: 0.5404\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.01460\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 23/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1628 - accuracy: 0.5514 - val_loss: 1.5671 - val_accuracy: 0.5467\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.01460\n",
      "Epoch 24/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1602 - accuracy: 0.5485 - val_loss: 1.6039 - val_accuracy: 0.5341\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.01460\n",
      "Epoch 25/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1648 - accuracy: 0.5511 - val_loss: 1.3103 - val_accuracy: 0.5507\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.01460\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 26/50\n",
      "673/673 [==============================] - 63s 94ms/step - loss: 1.1637 - accuracy: 0.5498 - val_loss: 1.5395 - val_accuracy: 0.5418\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.01460\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"./emotion_custom_vgg.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 10,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzyiY1bawJPf"
   },
   "source": [
    "## Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "colab_type": "code",
    "id": "aoH5OvNAwJPf",
    "outputId": "83589732-03da-42b3-cd94-983dd23bf1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[285  32  21  80  58  15]\n",
      " [ 93 123  21  75 124  92]\n",
      " [ 21  21 766  34  18  19]\n",
      " [116  44 157 140 103  66]\n",
      " [ 69  33  40 185 258   9]\n",
      " [ 12  37  25  17   5 320]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.48      0.58      0.52       491\n",
      "        Fear       0.42      0.23      0.30       528\n",
      "       Happy       0.74      0.87      0.80       879\n",
      "     Neutral       0.26      0.22      0.24       626\n",
      "         Sad       0.46      0.43      0.44       594\n",
      "    Surprise       0.61      0.77      0.68       416\n",
      "\n",
      "    accuracy                           0.54      3534\n",
      "   macro avg       0.50      0.52      0.50      3534\n",
      "weighted avg       0.51      0.54      0.52      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = classifier.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CReutMRyL2yn"
   },
   "source": [
    "## Loading Saved Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zx8BKDpF7VQY"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model(r'./emotion_custom_vgg.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEvbXuVYL2yr"
   },
   "source": [
    "## Testing the trained network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "h2B79oxJ-f7w",
    "outputId": "c06705f5-7cd2-4baa-d3cb-96c3152e64ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKir7ntHwJPi"
   },
   "source": [
    "## Visualizing Sample Validation Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(r'D:\\SummerInternship\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,0,255), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face\", (20,45) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,0,255), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
